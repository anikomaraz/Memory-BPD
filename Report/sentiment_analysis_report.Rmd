---
title: "Memory and BPD sentiment analysis"
author: "Tamas Nagy"
date: "January 29, 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidytext)
library(hunspell)
library(openxlsx)
library(readxl)
library(knitr)

source("Scripts/calculate_sentiment.R")

# This defines the groups that are related to the movies
groups <- tibble(group = c("gr1", "gr2", "gr3", "gr4", "gr5", "gr6", "gr7", "gr8", "gr9"),
                 group_affect = c(rep("positive", 3),
                                rep("neutral", 3),
                                rep("negative", 3)))

# Participants to exclude
sessions_to_exclude <- c(
  # Non-English response
  "qf3LHAoO3suS9hpjgDMR9uf-C-YnLaSlqCrkIVMZbpGTX4bZsVzU6bqevzaiZ80G"
  )

df_raw <- read_csv2("Data/data_final_long_scales_181121.csv")

```

# Process data on answer and 

```{r}
# Get all text data, put it into long format, and add time point and question
answer_df <-
    df_raw %>%
    # Exclude invalid sessions (see reasons above)
    filter(!session %in% sessions_to_exclude) %>% 
    select(session, matches("gr\\d+_qual")) %>% 
    gather(group_time, answer, -session, na.rm = TRUE) %>% 
    separate(group_time, c("group", "time"), sep = "_qual.t", convert = TRUE) %>% 
    # Add answer lenght
    mutate(answer_length = str_count(answer, "\\w+"))

# Tokenize text by word
word_df <- 
  answer_df %>% 
  unnest_tokens(word, answer)

```

# Correct the typos using a hand corrected dictionary of typos
```{r}
# Create a hand corrected dictionary of typos
corrections <- 
  read_excel("Data/text_data_for_typos.xlsx") %>% 
  # Unselect context as it only served for manual coding
  select(typo, corrected_word = word) %>% 
  drop_na() %>% 
  # Keep only unique rows
  distinct(.keep_all = TRUE)

# Correct the typos and re-tokenize
corrected_word_df <-
  word_df %>% 
  left_join(corrections, by = c("word" = "typo")) %>% 
  mutate(word = coalesce(corrected_word, word)) %>% 
  select(-corrected_word) %>% 
  # There are several missing space errors that require re-tokenizing
  unnest_tokens(word, word)
```

The number of corrected words is `r nrow(corrections)` (without duplicates).

# Calcualte sentiments 

```{r}
# Afinn returns a number that can be positive and negative, and reflects intensity
afinn_df <- 
  corrected_word_df %>% 
    # Add sentiments
    left_join(get_sentiments("afinn"), by = "word") %>% 
    group_by(session, group, time) %>% 
    # Get the summarised sentiment and the number of words in the answer
    summarise(answer_length = first(answer_length),
              sum_senti = sum(value, na.rm = TRUE),
              rel_senti = sum_senti/answer_length) %>% 
    ungroup() %>% 
    left_join(groups, by = "group") %>% 
    select(session, group, group_affect, everything())

# Bing, NRC and Loughran returns emotions and categories, and we don't do any weighting of 
# positive and negative emotions. But we do a relativization to the length of the answer.
# The number cannot get negative this way.
bing_df <-
  corrected_word_df %>% 
  calculate_sentiment(word, "bing") %>%
  gather(sentiment, score, word_negative:word_positive) %>% 
  mutate(sentiment = str_remove(sentiment, "word_")) %>% 
  group_by(session, group, time, sentiment) %>% 
  summarise(answer_length = first(answer_length),
            sum_senti = sum(score, na.rm = TRUE),
            rel_senti = sum_senti/answer_length) %>% 
  ungroup() %>% 
  left_join(groups, by = "group") %>% 
  select(session, group, group_affect, everything())

nrc_df <- 
  corrected_word_df %>% 
  calculate_sentiment(word, "nrc") %>%
  gather(sentiment, score, word_anger:word_trust, na.rm = TRUE) %>% 
  mutate(sentiment = str_remove(sentiment, "word_")) %>% 
  group_by(session, group, time, sentiment) %>% 
  summarise(answer_length = first(answer_length),
            sum_senti = sum(score, na.rm = TRUE),
            rel_senti = sum_senti/answer_length) %>% 
  ungroup()  %>% 
  left_join(groups, by = "group") %>% 
  select(session, group, group_affect, everything())
  

log_df <-
  corrected_word_df %>% 
  calculate_sentiment(word, "loughran") %>% 
  gather(sentiment, score, word_constraining:word_uncertainty, na.rm = TRUE) %>% 
  mutate(sentiment = str_remove(sentiment, "word_")) %>% 
  group_by(session, group, time, sentiment) %>% 
  summarise(answer_length = first(answer_length),
            sum_senti = sum(score, na.rm = TRUE),
            rel_senti = sum_senti/answer_length) %>% 
  ungroup()  %>% 
  left_join(groups, by = "group") %>% 
  select(session, group, group_affect, everything())

```


# Compare sentiment dictionaries. 
How many words are matched in the four dictionaries? We only look at positivity and 
negativity.

```{r}
bind_rows("nrc" = nrc_df, 
          "loughran" = log_df, 
          "bing" = bing_df,
          "afinn" = afinn_df,
          .id = "lexicon") %>% 
  filter(sentiment %in% c("positive", "negative") | is.na(sentiment)) %>% 
  filter(!sum_senti == 0) %>% 
  count(lexicon) %>% 
  kable()
```



